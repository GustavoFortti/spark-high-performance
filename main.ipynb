{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d90b74e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-06T19:47:07.233480Z",
     "iopub.status.busy": "2022-02-06T19:47:07.232915Z",
     "iopub.status.idle": "2022-02-06T19:47:07.402975Z",
     "shell.execute_reply": "2022-02-06T19:47:07.402264Z",
     "shell.execute_reply.started": "2022-02-06T19:47:07.233435Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import spark_partition_id, asc, desc\n",
    "from pyspark.sql.functions  import spark_partition_id\n",
    "from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n",
    "sc.setSystemProperty(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "# spark.serializer org.apache.spark.serializer.KryoSerializer\n",
    "# http://spark-configuration.luminousmen.com/\n",
    "# https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html\n",
    "# https://www.codetd.com/pt/article/10830437\n",
    "# https://databricks.com/session_na20/fine-tuning-and-enhancing-performance-of-apache-spark-jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIFO E FEAR\n",
    "# tunning\n",
    "# SPARK UI, paralelismo, particionamento, joins, serialização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe1ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.default.parallelism=140\n",
    "# spark.executor.memory=9g\n",
    "# spark.executor.instances=14\n",
    "# spark.driver.cores=5\n",
    "# spark.executor.cores=5\n",
    "# spark.driver.memory=9g\n",
    "# spark.driver.maxResultSize=9g\n",
    "# spark.driver.memoryOverhead=921m\n",
    "# spark.executor.memoryOverhead=921m\n",
    "# spark.dynamicAllocation.enabled=false\n",
    "# spark.sql.adaptive.enabled=true\n",
    "# spark.memory.fraction=0.8\n",
    "# spark.scheduler.barrier.maxConcurrentTasksCheck.maxFailures=5\n",
    "# spark.rdd.compress=true\n",
    "# spark.shuffle.compress=true\n",
    "# spark.shuffle.spill.compress=true\n",
    "# spark.serializer=org.apache.spark.serializer.KryoSerializer\n",
    "# spark.executor.extraJavaOptions=-XX:+UseG1GC -XX:+G1SummarizeConcMark\n",
    "# spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:+G1SummarizeConcMark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2f360fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-06T19:47:08.485181Z",
     "iopub.status.busy": "2022-02-06T19:47:08.484731Z",
     "iopub.status.idle": "2022-02-06T19:47:08.527143Z",
     "shell.execute_reply": "2022-02-06T19:47:08.526110Z",
     "shell.execute_reply.started": "2022-02-06T19:47:08.485143Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext\n",
    "# # Start the spark context using the SparkConf object named `conf` the extension created in your kernel.\n",
    "# sc=SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder\\\n",
    "#         .config('spark.extraListeners', 'sparkmonitor.listener.JupyterSparkMonitorListener')\\\n",
    "#         .config('spark.driver.extraClassPath', 'venv/lib/python3.7/site-packages/sparkmonitor/listener.jar')\\\n",
    "#         .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486f79a",
   "metadata": {},
   "source": [
    "# SKEW - analise de spark UI\n",
    "\n",
    "    spark.drives.cores\n",
    "    spark.driver.memory\n",
    "    executor-memory\n",
    "    executor-cores\n",
    "    spark.cores.max\n",
    "    \n",
    "    muita memória por executor pode resultar em atrasos excessivos no GC\n",
    "    pouca memória pode perder os benefícios da execução de várias tarefas em uma única JVM\n",
    "    analisar network, CPU, memoria\n",
    "    tempo de execução em cada task\n",
    "    numero de registros no dataframe\n",
    "    memoria e particonamento\n",
    "    \n",
    "    FIFO - primeiro a entrar, primeiro a sair\n",
    "    FAIR > FIFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64d26607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+----------+-------------------+\n",
      "|  a|  b|   c|         d|                  e|\n",
      "+---+---+----+----------+-------------------+\n",
      "|  1|4.0|GFG1|2000-08-01|2000-08-01 12:00:00|\n",
      "|  2|8.0|GFG2|2000-06-02|2000-06-02 12:00:00|\n",
      "|  4|5.0|GFG3|2000-05-03|2000-05-03 12:00:00|\n",
      "+---+---+----+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n",
      "Numero de partições: 8\n",
      "\n",
      "Numero de linhas por partição\n",
      "+-----------+-----+\n",
      "|partitionId|count|\n",
      "+-----------+-----+\n",
      "|          5|    1|\n",
      "|          7|    1|\n",
      "|          2|    1|\n",
      "+-----------+-----+\n",
      "\n",
      "Tamanho aproximado do dataframe: 2.8 MB\n",
      "Tamanho aproximado de cada partição: 0.35 MB\n",
      "\n",
      "spark ui ->\n",
      "http://192.168.0.15:4040\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=4., c='GFG1', d=date(2000, 8, 1),\n",
    "        e=datetime(2000, 8, 1, 12, 0)),\n",
    "   \n",
    "    Row(a=2, b=8., c='GFG2', d=date(2000, 6, 2),\n",
    "        e=datetime(2000, 6, 2, 12, 0)),\n",
    "   \n",
    "    Row(a=4, b=5., c='GFG3', d=date(2000, 5, 3),\n",
    "        e=datetime(2000, 5, 3, 12, 0))\n",
    "])\n",
    " \n",
    "# show table\n",
    "df.show()\n",
    " \n",
    "# show schema\n",
    "df.printSchema()\n",
    "\n",
    "# numero de partioções\n",
    "n_partitions = df.rdd.getNumPartitions()\n",
    "print(f'Numero de partições: {n_partitions}\\n')\n",
    "\n",
    "print('Numero de linhas por partição')\n",
    "df.withColumn(\"partitionId\", spark_partition_id()).groupBy(\"partitionId\").count().show()\n",
    "\n",
    "# df.withColumn(\"a\", spark_partition_id())\\\n",
    "#     .groupBy(\"a\")\\\n",
    "#     .count()\\\n",
    "#     .orderBy(asc(\"count\"))\\\n",
    "#     .show()\n",
    "\n",
    "def _to_java_object_rdd(rdd):  \n",
    "    \"\"\" Return a JavaRDD of Object by unpickling\n",
    "    It will convert each Python object into Java object by Pyrolite, whenever the\n",
    "    RDD is serialized in batch or not.\n",
    "    \"\"\"\n",
    "    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))\n",
    "    return rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)\n",
    "\n",
    "JavaObj = _to_java_object_rdd(df.rdd)\n",
    "\n",
    "nbytes = sc._jvm.org.apache.spark.util.SizeEstimator.estimate(JavaObj)\n",
    "mb = round(int(nbytes / df.rdd.getNumPartitions()) / 1024 / 1024, 2)\n",
    "print(f'Tamanho aproximado do dataframe: {mb} MB')\n",
    "size_partitions = mb / n_partitions\n",
    "print(f'Tamanho aproximado de cada partição: {size_partitions} MB')\n",
    "print('\\nspark ui ->')\n",
    "print(sc.uiWebUrl)\n",
    "# dados reais\n",
    "# working...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30c2cba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.startTime', '1644177423398'),\n",
       " ('spark.app.id', 'local-1644177424543'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.host', '192.168.0.15'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.sql.warehouse.dir', 'file:/home/magico/Main/spark-warehouse'),\n",
       " ('spark.driver.port', '33461'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
